{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde6c001-79a4-4766-8f4d-a073c4f0134e",
   "metadata": {},
   "source": [
    "# 1.Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2d0a8-289b-4437-ac08-0aaa44081614",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Etapas do Processo\n",
    "\n",
    "    1. Introdução\n",
    "        Explicação sobre os dados e o processo a ser realizado.\n",
    "        \n",
    "    2. Importação das Bibliotecas\n",
    "        Importação das bibliotecas que serão utilizadas no processo.\n",
    "        \n",
    "    3. Criação do Ambiente Spark\n",
    "        A criação de um ambiente Spark é o processo de configurar um ambiente de computação que utiliza o Apache Spark, um framework de processamento de dados em larga escala. Este ambiente pode incluir a instalação do Spark, configuração de variáveis de ambiente, e a integração com ferramentas como Jupyter Notebook, que permitem a interação com os dados de forma eficiente e intuitiva.\n",
    "        Os motivos para a decisão de trabalhar com spark são:\n",
    "        \n",
    "            * Processamento Distribuído: O Apache Spark é projetado para processar grandes volumes de dados de forma distribuída, o que significa que ele pode realizar operações em paralelo em clusters de computação, resultando em maior eficiência e velocidade.\n",
    "            \n",
    "           *  Flexibilidade: O Spark suporta diversas linguagens de programação, como Python, Java, Scala e R. Isso permite que os desenvolvedores escolham a linguagem com a qual estão mais confortáveis, facilitando a integração e a implementação de soluções de dados.\n",
    "            \n",
    "            * Diversas Funcionalidades: O ambiente Spark oferece uma gama de bibliotecas e módulos, como Spark SQL, MLlib (para aprendizado de máquina), Spark Streaming (para processamento de dados em tempo real) e GraphX (para processamento de gráficos), permitindo uma variedade de aplicações em diferentes áreas.\n",
    "            \n",
    "            * Integração com Big Data: O Spark é frequentemente utilizado em conjunto com sistemas de armazenamento de dados grandes, como Hadoop, Apache Cassandra e Amazon S3, permitindo o acesso e o processamento de dados em larga escala.\n",
    "            \n",
    "            * Desempenho: O Spark é otimizado para desempenho, com características como execução em memória, o que reduz o tempo necessário para ler e escrever dados em disco. Isso o torna mais rápido em comparação com outras soluções de processamento de dados.\n",
    "        \n",
    "            * Análise de Dados Interativa: A criação de um ambiente Spark, especialmente em conjunto com ferramentas como Jupyter Notebook, facilita a análise interativa de dados. Os usuários podem executar comandos e visualizar resultados em tempo real, promovendo uma melhor exploração dos dados.        \n",
    "            \n",
    "    4. Importação dos Dados\n",
    "        Nesta etapa serão importados os dados a serem trabalhados neste notebook.\n",
    "        \n",
    "    5. Análise Exploratória dos Dados\n",
    "        A análise exploratória dos dados é a etapa em que as tabelas importadas anteriormente serão analisadas quanto ao tipo dos metadados, à quantidade de dados, aos dados faltantes, às suas distribuições, entre outras informações fundamentais para iniciar qualquer tipo de trabalho.\n",
    "        \n",
    "    6. Tratamento dos Dados\n",
    "        Nesta etapa iremos realizar a feature selection que é o processo de identificar e selecionar as características (features) mais relevantes de um conjunto de dados para a construção de um modelo de machine learning. Essa etapa é crucial no pré-processamento dos dados, pois impacta diretamente na qualidade e desempenho do modelo. Os principais motivos de realizar a feature selection são: Redução da Dimensionalidade, Melhoria da Performance do Modelo, Interpretação e Explicabilidade, Aumento da Generalização e Facilitação da Visualização.\n",
    "        \n",
    "    7. Criação de Novo Arquivo\n",
    "        Ao final do processo, iremos gerar novos arquivos no formato .csv e .parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe500bd-d3b8-475c-a272-ee020309fcfd",
   "metadata": {},
   "source": [
    "Esses dados são fictícios e simulam as operações de uma empresa que gostaria de avaliar a perda de clientes (Churn).\n",
    "\n",
    "Considere os dados:\n",
    "\n",
    "    * Base de Churn com informações cadastrais de 1000 pessoas\n",
    "    * Base de Transações contendo as transações históricas de compras feitas por essas pessoas, incluindo detalhes como data da compra, valor gasto e categoria do produto\n",
    "\n",
    "Criar as seguintes variáveis explicativas:\n",
    "\n",
    "    * Tempo desde a Última Transação: A diferença entre a data mais recente no conjunto de dados e a última data de compra de cada cliente.\n",
    "    * Frequência de Compra: Quantidade de compras que o cliente fez dividida pelo número de meses desde sua inscrição.\n",
    "    * Total Gasto: Soma de todos os valores gastos pelo cliente em todas as suas transações.\n",
    "    * Categoria Favorita: Categoria de produto em que o cliente gastou a maior quantia.\n",
    "    * Gasto Médio por Transação: Total gasto dividido pelo número total de transações.\n",
    "    * Duração da Assinatura: Número de dias desde que o cliente se inscreveu até a data mais recente no conjunto de dados.\n",
    "    * Número de Categorias Compradas: Quantidade de categorias diferentes das quais o cliente comprou.\n",
    "    * Usou Suporte antes da Primeira Compra: Indicador (1 ou 0) se o cliente usou o suporte antes de fazer sua primeira compra.\n",
    "    * Dias entre Inscrição e Primeira Compra: Diferença em dias entre a data de inscrição do cliente e sua primeira transação.\n",
    "    * Frequência de Transações por Plano: Número de transações do cliente dividido pelos meses de inscrição, segmentado por plano (Básico, Intermediário, Avançado)\n",
    "\n",
    "Variáveis históricas - 3 meses:\n",
    "\n",
    "    * Média do Valor Gasto em Esportes nos Últimos 3 Meses: Esta variável calcula a média dos gastos do cliente na categoria \"Esportes\" nos últimos três meses.\n",
    "    * Média do Valor Gasto em Eletrônicos nos Últimos 3 Meses: Semelhante à anterior, mas focada na categoria \"Eletrônicos\".\n",
    "    * Média do Valor Gasto em Roupas nos Últimos 3 Meses: Foca na categoria \"Roupas\", representando a média dos gastos do cliente nos últimos três meses.\n",
    "    * Média do Valor Gasto em Alimentos nos Últimos 3 Meses: Representa a média dos gastos do cliente na categoria \"Alimentos\" nos últimos três meses.\n",
    "    * Média do Valor Gasto em Livros nos Últimos 3 Meses: Foca na categoria \"Livros\", calculando a média dos gastos do cliente nos últimos três meses.\n",
    "    Repita o passo anterior considerando - últimos 6 meses, 9 meses e 12 meses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf82e1-fa40-4a48-920b-8d50ece2e197",
   "metadata": {},
   "source": [
    "# 2.Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6563872-9eca-454d-9db8-a1b93a023c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\lilian\\miniconda3\\envs\\dsp_env\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\lilian\\miniconda3\\envs\\dsp_env\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in c:\\users\\lilian\\miniconda3\\envs\\dsp_env\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3be4f19-f35f-434e-8c59-4aaec12ccc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfc5aa-1e94-4358-a86d-209d989972ee",
   "metadata": {},
   "source": [
    "# 3.Criação do Ambiente Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d66dd763-eae9-409f-a8e6-9cbb07b03add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-RBQ4BAJ:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Feature Selection</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24ef5e1d3d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Feature Selection\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b6ba2f-a8f5-4636-86d1-f66ebf6c6482",
   "metadata": {},
   "source": [
    "# 4. Importação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0b9006e-0ecf-41f3-8523-8c8708e8d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------------------+------------+-------------+-----+\n",
      "| ID|Idade|Gênero|Dias desde a Inscrição|Usou Suporte|        Plano|Churn|\n",
      "+---+-----+------+----------------------+------------+-------------+-----+\n",
      "|  1|   21|     F|                  1331|           1|Intermediário|    1|\n",
      "|  2|   21|     M|                  1160|           0|Intermediário|    0|\n",
      "|  3|   62|     M|                   454|           1|       Básico|    0|\n",
      "|  4|   64|     M|                   226|           1|Intermediário|    0|\n",
      "|  5|   61|     M|                   474|           1|     Avançado|    0|\n",
      "+---+-----+------+----------------------+------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+----------+--------------------+------------------+-----------+\n",
      "|ID Transação|ID Cliente|                Data|             Valor|  Categoria|\n",
      "+------------+----------+--------------------+------------------+-----------+\n",
      "|           1|         1|2022-11-25 13:50:...|57.287427536330505|   Esportes|\n",
      "|           2|         1|2020-01-19 12:27:...| 97.07199340552512|  Alimentos|\n",
      "|           3|         1|2021-12-28 12:33:...|169.10581012381087|     Livros|\n",
      "|           4|         1|2022-02-05 01:39:...|199.38694865538451|     Roupas|\n",
      "|           5|         1|2022-11-16 23:06:...|160.00228343317622|Eletrônicos|\n",
      "+------------+----------+--------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_file_path = \"../data/base_churn.csv\"\n",
    "transaction_file_path = \"../data/base_transacoes.csv\"\n",
    "\n",
    "df_base = spark.read.csv(base_file_path, header=True, inferSchema=True)\n",
    "df_transaction = spark.read.csv(transaction_file_path, header=True, inferSchema=True)\n",
    "\n",
    "df_base.show(5)\n",
    "df_transaction.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc73084-f24f-4261-8961-e55a0f301519",
   "metadata": {},
   "source": [
    "# 5. Análise Exploratória dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a89cc2e7-cd0f-4843-81d7-1163e2cf24d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n##### Lista de coisas a serem feitas #####\\n\\nAjustar os nomes de cada coluna\\n, checar se os valores conferem com o tipo de cada coluna\\n, padronizar os nomes das colunas conforme seus tipos de metadados\\n, \\n, \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "##### Lista de coisas a serem feitas #####\n",
    "\n",
    "Ajustar os nomes de cada coluna\n",
    ", checar se os valores conferem com o tipo de cada coluna ok\n",
    ", padronizar os nomes das colunas conforme seus tipos de metadados\n",
    ", \n",
    ", \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "552a7c51-5bd0-4a98-859b-3486cb3d5f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Idade: integer (nullable = true)\n",
      " |-- Gênero: string (nullable = true)\n",
      " |-- Dias desde a Inscrição: integer (nullable = true)\n",
      " |-- Usou Suporte: integer (nullable = true)\n",
      " |-- Plano: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_base.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11802616-60d5-4fe8-8626-528d7cd66661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID Transação: integer (nullable = true)\n",
      " |-- ID Cliente: integer (nullable = true)\n",
      " |-- Data: timestamp (nullable = true)\n",
      " |-- Valor: double (nullable = true)\n",
      " |-- Categoria: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transaction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a635124b-089f-468b-acb9-f837422ce4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_base: 1000\n",
      "df_transaction: 10171\n"
     ]
    }
   ],
   "source": [
    "print(\"df_base:\", df_base.count())\n",
    "print(\"df_transaction:\", df_transaction.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f2cf5-3d73-408d-933e-f62f040fa69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305c718-93f4-41e0-b8fe-1def12408218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2d23151-0bf0-4186-ac6c-f8eda08bf393",
   "metadata": {},
   "source": [
    "# 6. Tratamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b14fa-d71f-457d-88d6-7a9e9ce41339",
   "metadata": {},
   "source": [
    "# 7. Criação de Novo Arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715be24-9645-4169-a0ba-3c5f31695db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
